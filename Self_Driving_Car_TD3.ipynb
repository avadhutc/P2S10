{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_Driving_Car_TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEh1VEkREKSTzJoZeek54l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avadhutc/P2S10/blob/master/Self_Driving_Car_TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ImuPmp-pXIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, latent_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.encoder = torch.nn.ModuleList([  \n",
        "            torch.nn.Conv2d(1, 8, 3), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            \n",
        "            torch.nn.Conv2d(8, 8, 3), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            \n",
        "            torch.nn.Conv2d(8, 16, 3, stride = 2), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.Conv2d(16, 16, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.Conv2d(16, 16, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.AdaptiveAvgPool2d((1, 1)),  \n",
        "            Flatten(),  \n",
        "        ])\n",
        "\n",
        "        self.linear = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(latent_dim+2, 16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(16, 8),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(8, action_dim),\n",
        "            \n",
        "        ])\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x, o):\n",
        "\n",
        "        for layer in self.encoder:\n",
        "                        \n",
        "            x = layer(x)\n",
        "            \n",
        "        counter = 0\n",
        "        for layer in self.linear:\n",
        "            counter += 1\n",
        "            if counter == 1:\n",
        "                x = torch.cat([x, o], 1) #concat orientation\n",
        "                \n",
        "                x = layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            \n",
        "            \n",
        "        x = self.max_action * torch.tanh(x)\n",
        "        \n",
        "        return x\n",
        "\t\t\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, latent_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.encoder_1 = torch.nn.ModuleList([  \n",
        "            torch.nn.Conv2d(1, 8, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            \n",
        "            torch.nn.Conv2d(8, 8, 3), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            \n",
        "            torch.nn.Conv2d(8, 16, 3, stride = 2), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.Conv2d(16, 16, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.Conv2d(16, 16, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            Flatten(),  ## \n",
        "        ])\n",
        "\n",
        "        self.linear_1 = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(latent_dim+2+action_dim, 16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(16, 8),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(8,1),\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.encoder_2 = torch.nn.ModuleList([  \n",
        "            torch.nn.Conv2d(1, 8, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            \n",
        "            torch.nn.Conv2d(8, 8, 3), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            \n",
        "            torch.nn.Conv2d(8, 16, 3, stride = 2), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.Conv2d(16, 16, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.Conv2d(16, 16, 3),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            Flatten(),  \n",
        "        ])\n",
        "        self.linear_2 = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(latent_dim+2+action_dim, 16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(16, 8),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(8,1),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, o, u):\n",
        "        \n",
        "        x1 = x\n",
        "        for layer in self.encoder_1:\n",
        "            x1 = layer(x1)\n",
        "            \n",
        "        counter = 0\n",
        "        for layer in self.linear_1:\n",
        "            counter += 1\n",
        "            \n",
        "            if counter == 1:\n",
        "                x1 = torch.cat([x1, o], 1) #concat orientation\n",
        "                x1 = torch.cat([x1, u], 1) #concat action\n",
        "                x1 = layer(x1)\n",
        "            else:\n",
        "                x1 = layer(x1)\n",
        "\n",
        "        x2 = x\n",
        "        for layer in self.encoder_2:\n",
        "            x2 = layer(x2)\n",
        "        counter = 0\n",
        "        for layer in self.linear_2:\n",
        "            counter += 1\n",
        "            if counter == 1:\n",
        "                x2 = torch.cat([x2, o], 1) #concat orientation\n",
        "                x2 = torch.cat([x2, u], 1) #concat action\n",
        "                x2 = layer(x2)\n",
        "            else:\n",
        "                x2 = layer(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "    def Q1(self, x, o, u):\n",
        "\n",
        "        for layer in self.encoder_1:\n",
        "            x = layer(x)\n",
        "\n",
        "        counter = 0\n",
        "        for layer in self.linear_1:\n",
        "            counter += 1\n",
        "            if counter == 1:\n",
        "                x = torch.cat([x, o], 1) #concat orientation\n",
        "                x = torch.cat([x, u], 1) #concat action\n",
        "                x = layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        return x\n",
        "        \n",
        "class TD3(object):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action, latent_dim):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action, latent_dim).to(device)\n",
        "        print(self.actor)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action, latent_dim).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "        self.critic = Critic(state_dim, action_dim, latent_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim, latent_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "        self.max_action = max_action\n",
        "        \n",
        "    def select_action(self, state, orientation):\n",
        "        state = state.unsqueeze(0).to(device) #add batch info\n",
        "        orientation = torch.Tensor(orientation).unsqueeze(0).to(device) #add batch info\n",
        "        #print(orientation.size())\n",
        "        return self.actor(state, orientation).cpu().data.numpy().flatten()\n",
        "\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, \\\n",
        "        discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        \n",
        "        for it in range(iterations):\n",
        "                \n",
        "            # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "            batch_states, batch_next_states, batch_orientation, batch_next_orientation, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "            state = torch.Tensor(batch_states).to(device)\n",
        "            next_state = torch.Tensor(batch_next_states).to(device)\n",
        "            orientation = torch.Tensor(batch_orientation).to(device)\n",
        "            next_orientation = torch.Tensor(batch_next_orientation).to(device)\n",
        "            action = torch.Tensor(batch_actions).to(device)\n",
        "            reward = torch.Tensor(batch_rewards).to(device)\n",
        "            done = torch.Tensor(batch_dones).to(device)\n",
        "            #print(\"iteration: \", it)\n",
        "            # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "            next_action = self.actor_target(next_state, next_orientation)\n",
        "            #print(\"enter target\")\n",
        "            \n",
        "            # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_orientation, next_action) #add orientation\n",
        "            \n",
        "            # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            \n",
        "            # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "            \n",
        "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "            current_Q1, current_Q2 = self.critic(state, orientation, action)\n",
        "            \n",
        "            # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "            \n",
        "            # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "            \n",
        "            # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "            if it % policy_freq == 0:\n",
        "                actor_loss = -self.critic.Q1(state, orientation, self.actor(state, orientation)).mean()\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "                    #print(\"successful\")\n",
        "\n",
        "                # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "    \n",
        "    # Making a save method to save a trained model\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "    \n",
        "    # Making a load method to load a pre-trained model\n",
        "    def load(self, filename, directory):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acUSe98cph75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "#define replay buffer\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        batch_states, batch_next_states, batch_orientation, batch_next_orientation, batch_actions, batch_rewards, batch_dones = [], [], [], [], [], [], []\n",
        "        for i in ind: \n",
        "            state, next_state, orientation, next_orientation, action, reward, done = self.storage[i]\n",
        "            #state, next_state, action, reward = self.storage[i]\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "            batch_next_states.append(np.array(next_state, copy=False))\n",
        "            batch_orientation.append(np.array(orientation, copy=False))\n",
        "            batch_next_orientation.append(np.array(next_orientation, copy=False))\n",
        "            batch_actions.append(np.array(action, copy=False))\n",
        "            batch_rewards.append(np.array(reward, copy=False))\n",
        "            batch_dones.append(np.array(done, copy=False))\n",
        "        return np.array(batch_states), np.array(batch_next_states),np.array(batch_orientation), np.array(batch_next_orientation), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2eSmFivph-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#:kivy 1.0.9\n",
        "# ref: https://kivy.org/docs/tutorials/pong.html\n",
        "\n",
        "<Car>:\n",
        "    size: 20, 10\n",
        "    origin: 10, 5\n",
        "    canvas:\n",
        "        PushMatrix\n",
        "        Rotate:\n",
        "            angle: self.angle\n",
        "            origin: self.center\n",
        "        Rectangle:\n",
        "            pos: self.pos\n",
        "            size: self.size\n",
        "            source: \"./images/car.png\"\n",
        "        PopMatrix\n",
        "\n",
        "\n",
        "\n",
        "<Game>:\n",
        "    car: game_car\n",
        "    \n",
        "    canvas:\n",
        "        Rectangle:\n",
        "            pos: self.pos\n",
        "            size: 1429, 660\n",
        "            source: \"./images/citymap.png\"\n",
        "\n",
        "    Car:\n",
        "        id: game_car\n",
        "        center: self.parent.center\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvYJ6ZxupiAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Self Driving Car\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "from random import random, randint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "#from torchvision import transforms\n",
        "import math\n",
        "\n",
        "\n",
        "# Importing the Kivy packages\n",
        "from kivy.app import App\n",
        "from kivy.uix.widget import Widget\n",
        "from kivy.uix.button import Button\n",
        "from kivy.graphics import Color, Ellipse, Line\n",
        "from kivy.config import Config\n",
        "from kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\n",
        "from kivy.vector import Vector\n",
        "from kivy.clock import Clock\n",
        "from kivy.core.image import Image as CoreImage\n",
        "from PIL import Image as PILImage\n",
        "from kivy.graphics.texture import Texture\n",
        "from PIL import ImageDraw\n",
        "# Importing the Dqn object from our AI in ai.py\n",
        "#rom ai import Dqn\n",
        "from td3_cnn1 import TD3\n",
        "from td3_utilities import ReplayBuffer\n",
        "import cv2\n",
        "from scipy import ndimage\n",
        "from PIL import Image\n",
        "import scipy\n",
        "\n",
        "import os \n",
        "\n",
        "\n",
        "# Adding this line if we don't want the right click to put a red point\n",
        "Config.set('input', 'mouse', 'mouse,multitouch_on_demand')\n",
        "Config.set('graphics', 'resizable', False)\n",
        "Config.set('graphics', 'width', '1429')\n",
        "Config.set('graphics', 'height', '660')\n",
        "\n",
        "\n",
        "# Introducing last_x and last_y, used to keep the last point in memory when we draw the sand on the map\n",
        "last_x = 0\n",
        "last_y = 0\n",
        "n_points = 0\n",
        "length = 0\n",
        "max_action = 40#15 #reduced to prevent steep turns\n",
        "save_models = True\n",
        "\n",
        "env_name = \"car_racing\"\n",
        "file_name = \"%s_%s\" % (\"TD3\", env_name)\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")\n",
        "  \n",
        "directory = \"pytorch_models\"\n",
        "\n",
        "\n",
        "#function to extract car image\n",
        "def extract_car(x, y, width, height, angle):\n",
        "        car_ = np.array([(0, 0), (width, 0), (width, height), (0, height), (0, 0)])\n",
        "        theta = (np.pi / 180.0) * angle\n",
        "        R = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                    [np.sin(theta), np.cos(theta)]])\n",
        "        car_offset = np.array([x, y])\n",
        "        cropped_car = np.dot(car_, R) + car_offset\n",
        "        return cropped_car\n",
        "#function to extract image and rotate\n",
        "def get_roi(img, angle, center, size, fill_with = 255):\n",
        "    angle = angle + 90\n",
        "    center[0] -= 0\n",
        "    img = np.pad(img, size, 'constant', constant_values = fill_with)\n",
        "    ##plt.imshow(img,cmap='gray')\n",
        "    ##plt.show()\n",
        "\n",
        "    a_0 = center[0]\n",
        "    a_1 = center[1]\n",
        "    img_tmp = PILImage.fromarray(img)#.astype(\"uint8\")*255)        \n",
        "    draw = ImageDraw.Draw(img_tmp)\n",
        "    extract_car_area = extract_car(x=int(a_1+80), y=int(a_0+80), width=10, height=20, angle = angle-90)#+180)\n",
        "    draw.polygon([tuple(p) for p in extract_car_area], fill=128)\n",
        "\n",
        "    init_size = 1.6*size\n",
        "    \n",
        "    center[0] += size\n",
        "    center[1] += size\n",
        "\n",
        "    img = np.asarray(img_tmp)\n",
        "    \n",
        "    cropped = img[int(center[0]-(init_size/2)) : int(center[0]+(init_size/2)) ,int(center[1]-(init_size/2)): int(center[1]+(init_size/2))]\n",
        "    \n",
        "    rotated = ndimage.rotate(cropped, angle, reshape = False, cval = 255.0)\n",
        "    y,x = rotated.shape\n",
        "    final = rotated[int(y/2-(size/2)):int(y/2+(size/2)),int(x/2-(size/2)):int(x/2+(size/2))]\n",
        "    \n",
        "    final = torch.from_numpy(np.array(final)).float().div(255)\n",
        "    final = final.unsqueeze(0).unsqueeze(0)\n",
        "    final = F.interpolate(final,size=(32,32))\n",
        "    \n",
        "    return final.squeeze(0)\n",
        "\n",
        "\n",
        "#initialise variables\n",
        "crop_dim = 80\n",
        "state_dim = 5 \n",
        "action_dim = 1\n",
        "latent_dim = 16\n",
        "brain = TD3(state_dim,action_dim,max_action,latent_dim)\n",
        "replay_buffer = ReplayBuffer()\n",
        "last_reward = 0\n",
        "scores = []\n",
        "im = CoreImage(\"./images/MASK1.png\")\n",
        "mask = cv2.imread('./images/mask.png',0)\n",
        "#initialising variables for training:\n",
        "seed = 0 # Random seed number\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 500000 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "start_timesteps = 10000 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "batch_size = 30 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
        "expl_noise = 0.4\n",
        "total_timesteps = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()\n",
        "#max_timesteps = 100000\n",
        "state = torch.zeros([1,state_dim,state_dim]) #shape of the cropped image\n",
        "episode_reward = 0\n",
        "episode_timesteps = 0\n",
        "sand_counter = 0\n",
        "p_sand = 0\n",
        "p_living = 0\n",
        "lp_counter = 0\n",
        "#decay expl noise every 4000 timestep\n",
        "expl_noise_vals = np.linspace(0.1, int(max_action/1000), num=int(max_timesteps/4000), endpoint=True, retstep=False, dtype=None, axis=0) # Exploration noise - STD value of exploration Gaussian noise\n",
        "reward_window = []\n",
        "log_interval = 10  # print avg reward after interval\n",
        "\n",
        "# Initializing the environment\n",
        "first_update = True\n",
        "def init():\n",
        "    global sand\n",
        "    global goal_x\n",
        "    global goal_y\n",
        "    global first_update\n",
        "    sand = np.zeros((longueur,largeur))\n",
        "    img = PILImage.open(\"./images/mask.png\").convert('L')\n",
        "    sand = np.asarray(img)/255\n",
        "    #sand = np.pad(sand, 160, 'constant', constant_values = 1)\n",
        "    goal_x = 1420\n",
        "    goal_y = 622\n",
        "    first_update = False\n",
        "    global swap\n",
        "    swap = 0\n",
        "\n",
        "\n",
        "# Initializing the last distance\n",
        "last_distance = 0\n",
        "\n",
        "\n",
        "# Creating the car class\n",
        "class Car(Widget):\n",
        "    \n",
        "    angle = NumericProperty(0)\n",
        "    rotation = NumericProperty(0)\n",
        "    velocity_x = NumericProperty(0)\n",
        "    velocity_y = NumericProperty(0)\n",
        "    velocity = ReferenceListProperty(velocity_x, velocity_y)\n",
        "    \n",
        "    def move(self, rotation):\n",
        "        #signals have been removed from any computaion for TD3, but are still visible\n",
        "        self.pos = Vector(*self.velocity) + self.pos\n",
        "        self.rotation = rotation\n",
        "        self.angle = self.angle + self.rotation\n",
        "        \n",
        "\n",
        "\n",
        "# Creating the game class\n",
        "\n",
        "class Game(Widget):\n",
        "    car = ObjectProperty(None)\n",
        "   \n",
        "    def serve_car(self):\n",
        "        self.car.center = self.center\n",
        "        self.car.velocity = Vector(6, 0)\n",
        "\n",
        "    def update(self, dt):\n",
        "        \n",
        "        global brain\n",
        "        global reward\n",
        "        global scores\n",
        "        global last_distance\n",
        "        global goal_x\n",
        "        global goal_y\n",
        "        global longueur\n",
        "        global largeur\n",
        "        global swap\n",
        "        global orientation\n",
        "        global last_action\n",
        "        global last_distance_travelled\n",
        "        global start_timesteps\n",
        "        global batch_size\n",
        "        global discount\n",
        "        global tau\n",
        "        global policy_noise\n",
        "        global noise_clip\n",
        "        global policy_freq\n",
        "        global expl_noise\n",
        "        global reward_window\n",
        "        global total_timesteps\n",
        "        global episode_num\n",
        "        global done\n",
        "        global t0\n",
        "        global max_timesteps\n",
        "        global state\n",
        "        global episode_reward\n",
        "        global episode_timesteps\n",
        "        global sand_counter\n",
        "        global p_sand\n",
        "        global p_living\n",
        "        global lp_counter\n",
        "        #decay expl noise every 4000 timestep\n",
        "        global expl_noise_vals\n",
        "        global crop_dim\n",
        "\n",
        "        log_f = open(\"training_log.txt\", \"a+\")\n",
        " \n",
        "        longueur = self.width\n",
        "        largeur = self.height\n",
        "        #state = np.zeros(5)\n",
        "        sand_time = []\n",
        "\n",
        "        if first_update:\n",
        "            init()\n",
        "\n",
        "        \n",
        "        # We start the main loop over 500,000 timesteps\n",
        "        if total_timesteps < max_timesteps:\n",
        "            # If the episode is done\n",
        "            if done:\n",
        "                # If we are not at the very beginning, we start the training process of the model\n",
        "                if total_timesteps != 0:\n",
        "                    #print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(self.total_timesteps,self.episode_num, self.episode_reward))\n",
        "                    distance_travelled = np.sqrt((self.car.x - 715)**2 + (self.car.y - 360)**2)\n",
        "                    distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2)\n",
        "                    s_reward = round(float(episode_reward * p_sand/(p_sand+p_living)),2)\n",
        "                    l_reward = round(float(episode_reward * p_living/(p_sand+p_living)),2)\n",
        "                    \n",
        "                    print(\"Time-Steps: \", total_timesteps, \"Episode-Num: \",episode_num, \"Episode-Reward: \", episode_reward)\n",
        "                    print(\"Episode-Steps: \", episode_timesteps,\"Traveled-Distance: \", round(float(distance_travelled),2))\n",
        "                    print(\"Remaining-Distance: \", round(float(distance),2), \"Sand-Penalty: \", s_reward, \"Living-Penalty: \", l_reward)\n",
        "\n",
        "                    log_f.write(\"Time-Steps: {}\\t Episode-Num: {}\\t Episode-Reward: {}\\n\".format(total_timesteps, episode_num, episode_reward))\n",
        "                    log_f.write(\"Episode-Steps: {}\\t Traveled-Distance: {}\\t Remaining-Distance: {}\\n\".format(episode_timesteps, round(float(distance_travelled),2), round(float(distance),2)))\n",
        "                    log_f.write(\"Sand-Penalty: {}\\t Living-Penalty: {}\\n\".format(s_reward, l_reward))\n",
        "                    log_f.flush()          \n",
        "                if total_timesteps > start_timesteps:\n",
        "                    \n",
        "                    brain.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "                    print(\"Training-Mode..\")                    \n",
        "                \n",
        "                pos_index = np.random.randint(0,3)\n",
        "\n",
        "                if pos_index == 0:\n",
        "                    #update car position\n",
        "                    self.car.x = 715 #\n",
        "                    self.car.y = 360 #\n",
        "                    self.car.angle = 0\n",
        "                \n",
        "                elif pos_index == 1:\n",
        "                    self.car.x = 137#+\n",
        "                    self.car.y = 280 #+\n",
        "                    self.car.angle = 0\n",
        "                elif pos_index == 2:\n",
        "                    self.car.x = 715#+ \n",
        "                    self.car.y = 540 #+ \n",
        "                    self.car.angle = 0\n",
        "                \n",
        "\n",
        "                self.car.velocity = Vector(6, 0)\n",
        "                xx = goal_x - self.car.x\n",
        "                yy = goal_y - self.car.y\n",
        "                orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
        "                orientation = [orientation, -orientation]\n",
        "\n",
        "                #initialise 1st state after done, move it towards orientaation\n",
        "                \n",
        "                state = get_roi(mask, self.car.angle, [self.car.x, self.car.y], crop_dim)\n",
        "                \n",
        "                done = False\n",
        "                last_action = [0]\n",
        "                last_distance_travelled = 0\n",
        "                # Set rewards and episode timesteps to zero\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                episode_num += 1\n",
        "                sand_counter = 0\n",
        "                lp_Counter = 0\n",
        "                p_living = 0\n",
        "                p_sand = 0\n",
        "            # Before 10000 timesteps, we play random actions based on uniform distn\n",
        "            if total_timesteps < start_timesteps:\n",
        "                action = [random.uniform(-max_action * 1.0, max_action * 1.0)]\n",
        "                \n",
        "            else:\n",
        "            \n",
        "                action = brain.select_action(state, np.array(orientation))\n",
        "                print(\"Orginal-action:\", action)\n",
        "                action = (action + np.random.normal(0, expl_noise)).clip(-max_action, max_action)\n",
        "                print(\"Noise-action:\", action)                     \n",
        "\n",
        "            #The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "            #debug\n",
        "            if type(action) != type([]):\n",
        "                self.car.move(action.tolist()[0])\n",
        "            else:\n",
        "                self.car.move(action[0])\n",
        "            distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2)\n",
        "                     \n",
        "            \n",
        "            sand_time = []\n",
        "            \n",
        "            # evaluating reward and done\n",
        "            \n",
        "            if sand[int(self.car.x),int(self.car.y)] > 0:# and self.total_timesteps < start_timesteps:\n",
        "                self.car.velocity = Vector(0.5, 0).rotate(self.car.angle)\n",
        "                sand_counter +=1\n",
        "                reward = -1\n",
        "                done = False\n",
        "                p_sand += 0.1\n",
        "\n",
        "            else: # otherwise\n",
        "                self.car.velocity = Vector(2, 0).rotate(self.car.angle)\n",
        "                sand_counter = 0\n",
        "                reward = -0.2 #living penalty\n",
        "                p_living += 0.1\n",
        "\n",
        "                if distance < last_distance:\n",
        "                    reward = 0.1\n",
        "                    p_living -= 1\n",
        "        \n",
        "            if (self.car.x < 5) or (self.car.x > self.width - 5) or (self.car.y < 5) or (self.car.y > self.height - 5): #crude way to handle model failing near boundaries\n",
        "                done = True\n",
        "                reward = -1\n",
        "                p_living += 1\n",
        "            \n",
        "            if distance < 100:                \n",
        "                reward = 1\n",
        "                if swap == 1:\n",
        "                    goal_x = 1420\n",
        "                    goal_y = 622\n",
        "                    swap = 0\n",
        "                    \n",
        "                else:\n",
        "                    goal_x = 9\n",
        "                    goal_y = 85\n",
        "                    swap = 1\n",
        "                    \n",
        "            last_distance = distance\n",
        "            new_state = get_roi(mask, self.car.angle, [self.car.x, self.car.y], crop_dim)\n",
        "            xx = goal_x - self.car.x\n",
        "            yy = goal_y - self.car.y\n",
        "            new_orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
        "            new_orientation = [new_orientation, -new_orientation]\n",
        "            \n",
        "            \n",
        "            distance_travelled = np.sqrt((self.car.x - 715)**2 + (self.car.y - 360)**2)\n",
        "            \n",
        "            reward_window.append(reward)\n",
        "\n",
        "            if sum(reward_window[len(reward_window)-100:]) <= -99 or episode_timesteps % 2500 == 0 and episode_timesteps != 0:\n",
        "                done = True\n",
        "                reward = -2\n",
        "                reward_window = []\n",
        "\n",
        "            #end episode if more time on sand\n",
        "            if sand_counter == 20:\n",
        "                done = True\n",
        "            \n",
        "            # We increase the total reward\n",
        "            episode_reward += reward\n",
        "\n",
        "            sand_time.append(sand_counter) #not used\n",
        "\n",
        "\n",
        "            # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "            replay_buffer.add((state, new_state, orientation, new_orientation, action, reward, done))\n",
        "            #print(self.state, new_state, action, reward, self.done)\n",
        "            state = new_state\n",
        "            orientation = new_orientation\n",
        "            episode_timesteps += 1\n",
        "            total_timesteps += 1\n",
        "            last_action = action\n",
        "            last_distance_travelled = distance_travelled\n",
        "            \n",
        "            \n",
        "            if total_timesteps % 100 == 0:\n",
        "                if not os.path.exists(directory):\n",
        "                    os.mkdir(directory)\n",
        "                brain.save(file_name, directory)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "class MyPaintWidget(Widget):\n",
        "\n",
        "    def on_touch_down(self, touch):\n",
        "        global length, n_points, last_x, last_y\n",
        "        with self.canvas:\n",
        "            Color(0.8,0.7,0)\n",
        "            d = 10.\n",
        "            touch.ud['line'] = Line(points = (touch.x, touch.y), width = 10)\n",
        "            last_x = int(touch.x)\n",
        "            last_y = int(touch.y)\n",
        "            n_points = 0\n",
        "            length = 0\n",
        "            sand[int(touch.x),int(touch.y)] = 1\n",
        "            img = PILImage.fromarray(sand.astype(\"uint8\")*255)\n",
        "            img.save(\"./images/sand.jpg\")\n",
        "\n",
        "    def on_touch_move(self, touch):\n",
        "        global length, n_points, last_x, last_y\n",
        "        if touch.button == 'left':\n",
        "            touch.ud['line'].points += [touch.x, touch.y]\n",
        "            x = int(touch.x)\n",
        "            y = int(touch.y)\n",
        "            length += np.sqrt(max((x - last_x)**2 + (y - last_y)**2, 2))\n",
        "            n_points += 1.\n",
        "            density = n_points/(length)\n",
        "            touch.ud['line'].width = int(20 * density + 1)\n",
        "            sand[int(touch.x) - 10 : int(touch.x) + 10, int(touch.y) - 10 : int(touch.y) + 10] = 1\n",
        "\n",
        "            \n",
        "            last_x = x\n",
        "            last_y = y\n",
        "\n",
        "# Adding the API Buttons (clear, save and load)\n",
        "\n",
        "class CarApp(App):\n",
        "\n",
        "    def build(self):\n",
        "        parent = Game()\n",
        "        parent.serve_car()\n",
        "        #Clock.max_iteration = 5\n",
        "        Clock.schedule_interval(parent.update, 1.0/60.0)\n",
        "        self.painter = MyPaintWidget()\n",
        "        clearbtn = Button(text = 'clear')\n",
        "        savebtn = Button(text = 'save', pos = (parent.width, 0))\n",
        "        loadbtn = Button(text = 'load', pos = (2 * parent.width, 0))\n",
        "        clearbtn.bind(on_release = self.clear_canvas)\n",
        "        savebtn.bind(on_release = self.save)\n",
        "        loadbtn.bind(on_release = self.load)\n",
        "        parent.add_widget(self.painter)\n",
        "        parent.add_widget(clearbtn)\n",
        "        parent.add_widget(savebtn)\n",
        "        parent.add_widget(loadbtn)\n",
        "        return parent\n",
        "\n",
        "    def clear_canvas(self, obj):\n",
        "        global sand\n",
        "        self.painter.canvas.clear()\n",
        "        sand = np.zeros((longueur,largeur))\n",
        "\n",
        "    def save(self, obj):\n",
        "        print(\"saving brain...\")\n",
        "        brain.save(file_name, \"pytorch_models\")\n",
        "        #plt.plot(scores)\n",
        "        #plt.show()\n",
        "\n",
        "    def load(self, obj):\n",
        "        print(\"loading last saved brain...\")\n",
        "        brain.load(file_name, \"pytorch_models\")\n",
        "\n",
        "# Running the whole thing\n",
        "if __name__ == '__main__':\n",
        "    CarApp().run()\n",
        "    #f.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLmK98YHpiCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5KwDG-PpiFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyAyBzyWpiHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fgxkZd4piJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}